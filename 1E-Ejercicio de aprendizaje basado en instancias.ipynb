{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje basado en instancias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos sobre c√°ncer de mama est√° incluido en *Scikit-learn*, se obtiene usando la funci√≥n `load_breast_cancer` incluida en la librer√≠a `sklearn.datasets`. Este conjunto de datos contiene 569 ejemplos con 30 caracter√≠sticas sobre clasificaciones de c√°ncer de mama, con dos clasificaciones posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos es un diccionario con varios campos:\n",
    "* `data`: Es el conjunto de datos, se trata de un array en el que cada componente es un array con las caracter√≠sticas de cada instancia.\n",
    "* `target`: Es el conjunto de valores de clasificaci√≥n para cada instancia. Es un array del mismo tama√±o que `data`, en el que se indica el valor de clasificaci√≥n de cada instancia, en el mismo orden en que √©stas se encuentran en el array `data`.\n",
    "* `DESCR`: Es una descripci√≥n del conjunto de datos.\n",
    "* `target_names`: Es un array con los nombres de cada valor de clasificaci√≥n.\n",
    "* `feature_names`: Es un array con los nombres de cada caracter√≠stica.\n",
    "\n",
    "Almacenamos los datos en las variables `X_data`, `y_data`, `X_names` e `y_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data, X_names, y_names = \\\n",
    "    cancer.data, cancer.target, cancer.feature_names, cancer.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos aislados (*outliers*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo aislado (*outlier* en ingl√©s) es un ejemplo de entrenamiento que est√° rodeado por ejemplos que no pertenecen a su misma clase. La presencia de ejemplos aislados en un conjunto de entrenamiento puede deberse a:\n",
    "\n",
    "* Un error en los datos\n",
    "* Una cantidad insuficiente de ejemplos de la clase de los ejemplos aislados\n",
    "* La existencia de caracter√≠sticas que no se est√°n teniendo en cuenta\n",
    "* Clases desbalanceadas \n",
    "\n",
    "En el contexto de la clasificaci√≥n basada en instancias mediante el algoritmo **k**-*NN*, los ejemplos aislados generan ruido que disminuye el rendimiento del clasificador. Dados dos n√∫meros naturales, $k$ y $r$, con $k>r$, decimos que un ejemplo es ($k$,$r$)-aislado si en sus $k$ vecinos m√°s cercanos hay m√°s de $r$ ejemplos que no pertenecen a su misma clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio vamos a identificar ejemplos ($k$,$r$)-aislados en el conjunto de datos y vamos a comparar el rendimiento del clasificador **K**-*NN* con y sin estos ejemplos. Para ello vamos a utilizar la clase `NearestNeighbors` de *Scikit-learn*, incluida en la librer√≠a `sklearn.neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejercicio consiste en\n",
    "\n",
    "* Investigar la clase `NearestNeighbors` de *Scikit-learn* y c√≥mo se usa para obtener los vecinos m√°s cercanos a un ejemplo dado dentro de un conjunto de datos, con respecto a una medida de distancia.\n",
    "* Definir la funci√≥n `buscaOutliers` que dados dos n√∫meros naturales `k` y `r`, devuelve la lista de los √≠ndices de los ejemplos del conjunto de entrenamiento sobre el c√°ncer que son (`k`,`r`)-aislados. Es decir, la lista de los √≠ndices de los ejemplos del conjunto de entrenamiento `X_data` para los que en sus `k` vecinos m√°s cercanos hay m√°s de `r` ejemplos con una clasificaci√≥n distinta a la del propio ejemplo.\n",
    "* Construir un segundo conjunto de datos `Xo_data` obtenido a partir del inicial eliminando todos los ejemplos (5,3)-aislados.\n",
    "* Construir dos modelos de decisi√≥n (para valores de `p` y `k` coherentes con la b√∫squeda de ejemplos aislados), uno para cada conjunto de datos considerado `X_data` y `Xo_data`, realizando en ambos casos una separaci√≥n del $30$% de datos de prueba y $70$% de datos de entrenamiento.\n",
    "* Comparar el rendimiento de ambos modelos sobre su correspondiente conjunto de prueba.\n",
    "\n",
    "El **desarrollo tiene que estar razonado**, indicando en cada apartado qu√© se est√° haciendo, **demostrando as√≠ el conocimiento adquirido en este m√≥dulo**. ¬øQu√© conclusiones puedes sacar de lo aprendido sobre aprendizaje basado en instancias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase **NearestNeighbors** posee los m√©todos necesarios para hallar la serie de vecinos de un miembro del conjunto. \n",
    "B√°sicamente crearemos un objeto de este tipo mediante su constructor, que no tiene par√°metros obligatorios, pero con el cual podemos definir:\n",
    "\n",
    "* **n_neighbors**: Por defecto = 5. Es el n√∫mero de vecinos que ser√°n determinantes en la b√∫squeda, el **k** de nuestro algoritmo **knn**.\n",
    "* **radius**: Por defecto = 1.0. Lo usamos si queremos que el radio de distancia respecto al miembro del conjunto sea determinante y no lo sea √∫nicamente el n√∫mero de vecinos sin importar su distancia.\n",
    "* **algorithm**: Para definir qu√© algoritmo queremos utilizar. Por defecto es 'auto', que determina el algoritmo autom√°ticamente en base a los datos de aprendizaje.\n",
    "* **otros**: El contructor de NearestNeighbors puede utilizar otros par√°metros como **metric** o **p**, ya visto anteriormente en el m√≥dulo para seleccionar el m√©todo de medici√≥n de distancias. \n",
    "\n",
    "Para el presente ejercicio s√≥lo usaremos **n_neighbors** para definir nuestra **k**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro aspecto importante de la clase **NearestNeighbors** ser√°n sus m√©todos **fit()** y **kneighbors()**.\n",
    "\n",
    "* **fit()**: A este m√©todo le pasaremos nuestro conjunto de datos completo y nos servir√° para entrenar el algortimo.\n",
    "* **kneighbors()**: Con este m√©todo podemos encontrar los vecinos del miembro que pasamos como argumento. Pasamos el conjunto de datos, el k deseado y por √∫ltimo especificamos si queremos que se a√±adan las distancias al array resultante.\n",
    "\n",
    "Veamos un ejemplo simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Nuestro conjunto de datos\n",
    "samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n",
    "\n",
    "#Definimos el objeto con k=2 y radio=0.4\n",
    "neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n",
    "#Entrenamos\n",
    "neigh.fit(samples)\n",
    "\n",
    "#¬øCu√°les son los 2 vecinos m√°s cercanos de [0, 0, 1.3] en samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]?\n",
    "neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, el m√©todo devuelve un conjunto de √≠ndices. Los √≠ndices 2 y 0 nos muestran que [0, 0, 1] y [0, 0, 2] son los 2 vecinos m√°s cercanos a [0, 0, 1.3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear el m√©todo **buscaOutliers**, que encuentra los elementos **(ùëò,ùëü)-aislado** del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este m√©todo vamos a recorrer **X_data** de principio a fin. Para cada uno de sus elemento comprobaremos de entre sus **k-vecinos** cu√°les son de un tipo diferente al del elemento de la instancia actual. En el ejercicio se ha tenido en cuenta que **kneighbors** devuelve tambi√©n el propio elemento de la instancia puesto que su distancia es 0. Por ello, de manera interna, usaremos k+1 al buscar los vecinos y posteriormente borraremos el primer elemento de la lista, siendo √©ste el propio elemento de la instancia, que es considerado trivial para determinar la informaci√≥n que deseamos encontrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def buscaOutliers(k,r):\n",
    "    #Defino NearestNeighbors para k+1 vecinos y lo entreno con todo mi conjunto de datos\n",
    "    neigh = NearestNeighbors(n_neighbors=k+1)\n",
    "    neigh.fit(X_data, y_data)\n",
    "    outliers_array = []\n",
    "    for objetivo in range(len(X_data)):\n",
    "        #Encuentro los k+1 vecinos del actual objetivo\n",
    "        vecinos_posible_outlier = neigh.kneighbors([X_data[objetivo]],return_distance=False)\n",
    "        #Elimino el primer elemento del array, es decir, el objetivo en s√≠ mismo\n",
    "        vecinos_posible_outlier2 = np.delete(vecinos_posible_outlier,0)\n",
    "        #Recojo en un array el conjunto de datos de los vecinos\n",
    "        vecinos_data = y_names[y_data[vecinos_posible_outlier2]]\n",
    "        #Compruebo de entre los vecinos, cu√°les son del mismo tipo que el objetivo. \n",
    "        #Si el total es menor que k-r, objetivo es outlier\n",
    "        if(np.count_nonzero(vecinos_data == y_names[y_data[objetivo]]) < (k-r)):\n",
    "            outliers_array.append(objetivo)        \n",
    "    return outliers_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busquemos los **outliers** de nuestro conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 14, 26, 38, 39, 41, 86, 91, 92, 99, 135, 146, 157, 190, 194, 209, 215, 297, 363, 379, 385, 430, 465, 491, 536]\n"
     ]
    }
   ],
   "source": [
    "outliers = buscaOutliers(5,3)\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√âste ser√≠a el conjunto de √≠ndices indicando todos los elementos pertenecientes a nuestro datos que son sonsiderados **outliers** para **k=5** y **r=3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para contruir **Xo_data** basta con eliminar los elementos **outliers** del conjunto **X_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xo_data = np.delete(X_data,outliers, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n debemos eliminar los elementos de **y_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo_data = np.delete(y_data,outliers, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto queda listo nuestro conjunto de datos **Xo_data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear los modelos de decisi√≥n de **X_data** y **Xo_data**. Para ello usaremos la librer√≠a **model_selection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos con el conjunto **x_data**, haremos una divisi√≥n 70/30 en los datos de entrenamiento/prueba. Bastar√° con que especifiquemos el valor de **0.30** para test_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X_data,y_data,test_size = 0.30,\n",
    "                   random_state=4861,stratify=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n usaremos la clase **KNeighborsClassifier** para la construcci√≥n del modelo. √âsto nos permitir√° utilizar los m√©todos necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos **k=5** tal y como hicimos al buscar los outliers. Tambi√©n respetaremos el m√©todo de medici√≥n de distancias, distancias euclideas, usado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=5,p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo con los datos de training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que el modelo funciona de forma correcta podemos definir el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siendo **knn1.predict(X_test)** la predicci√≥n de resultados que genera nuestro modelo en base a los datos de training. \n",
    "Podemos comparar este resultado con los datos de prueba **y_test**, caso ideal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_clas = knn1.predict(X_test)\n",
    "y_clas[y_clas != y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, la predicci√≥n ha dado un resultado distinto al preteneciente a **y_test** en 10 elementos.\n",
    "Sabiendo la cantidad de elementos de **y_test** con la funci√≥n **shape()** y sabiendo que la predicci√≥n coincide en esta cantidad restando 10 elementos podemos concluir lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415204678362573"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rendimiento = (y_test.shape[0]-10)/y_test.shape[0]\n",
    "rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, modelo para **X_data** tiene un rendimiento del **0.94%** aproximadamente.\n",
    "Estas √∫ltimas operaciones pueden resumirse en la funci√≥n **score()** como vemos a continuaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415204678362573"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actuaremos de forma an√°loga para construir el modelo de **Xo_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xo_train, Xo_test, yo_train, yo_test = \\\n",
    "  train_test_split(Xo_data,yo_data,test_size = 0.30,\n",
    "                   random_state=4861,stratify=yo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors=5,p=2)\n",
    "knn2.fit(Xo_train,yo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2.score(Xo_test,yo_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un rendimiento de **1.0** indica que no se han cometido errores en la predicci√≥n, es decir, tenemos un rendimiento del **100%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo vamos a comparar el rendimiento de los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento de X_data: 0.9415204678362573\n",
      "Rendimiento de Xo_data: 1.0\n",
      "Diferencia de porcentaje 0.05847953216374269\n"
     ]
    }
   ],
   "source": [
    "rendimiento_X_data = knn1.score(X_test,y_test)\n",
    "rendimiento_Xo_data = knn2.score(Xo_test,yo_test)\n",
    "\n",
    "print('Rendimiento de X_data:',rendimiento_X_data)\n",
    "print('Rendimiento de Xo_data:',rendimiento_Xo_data)\n",
    "print('Diferencia de porcentaje', rendimiento_Xo_data-rendimiento_X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de **Xo_data** es casi 6 puntos mayor en rendimiento al modelo de **X_data**, podemos concluir que el procesamiento de **outliers** ha sido muy fruct√≠fero en esta ocasi√≥n puesto que hemos obtenido un segundo modelo, no s√≥lo m√°s eficiente, sino del **100%** de rendimiento posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje basado en instancias es muy √∫til cuando tenemos datos est√°ticos, tipos diferenciados y poco ruido. El algoritmo **Knn** no es un concepto nuevo al menos respecto a ingenier√≠a de computadores pero s√≠ lo es el uso de √©ste con tuplas complejas de datos y el tener en cuenta la influencia de **outliers**. Es por ello que las principales conclusi√≥nes de lo aprendido son ganar la capacidad de sacar provecho al algoritmo en casos reales y poder conocer de forma desgranada el porqu√© del rendimiento de un modelo concreto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
