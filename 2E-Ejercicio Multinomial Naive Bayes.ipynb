{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos probabilísticos (Ejercicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de Naive Bayes multinomial a la detección de SMS *spam*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio se pide reproducir lo realizado en el caso práctico que se ha descrito en los vídeos (análisis de sentimiento en críticas de cine), pero ahora para detectar cuándo un mensaje corto (SMS) es *spam*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos consiste una serie de mensajes SMS (5574 en total), que están clasificados como mensajes basura (*spam*) o mensajes normales (*ham*). Los datos se pueden obtener en el [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). \n",
    "\n",
    "En concreto, descargar el fichero [smsspamcollection.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip), y descomprimirlo para obtener un fichero de texto SMSSpamCollection. En este fichero de texto hay una línea por cada sms, con el formato: *clase* *tabulador* *sms*. Por ejemplo, la primera línea es:\n",
    "\n",
    "`ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...`\n",
    "\n",
    "El fichero debe ser leído convenientemente para poder aplicar la vectorización. Se puede hacer la lectura usando las funciones python de lectura de ficheros, pero se recomienda usar la instrucción `read_table` de la biblioteca `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pandas* es una biblioteca de python muy utilizada para manipular y analizar datos. Si el fichero se lee con la orden `read_table` (se pide averiguar la manera concreta de hacerlo), entonces se obtendrá una tabla (o *Data Frame*), en el que las etiquetas serán una columna y los correspondientes sms otra. Esto permite obtener de manera sencilla la lista de etiquetas o clases, y por otro lado la lista de mensajes, en el mismo orden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendiendo a clasificar SMSs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pide reproducir con estos datos lo realizado en el *notebook* en el que se aplica Naive Bayes Multinomial al análisis de sentimientos de críticas de cine, pero ahora para clasificar un SMS como *spam* o como normal. Esto incluye:\n",
    "\n",
    "* Separación de los textos en entrenamiento y prueba \n",
    "* Vectorización de los textos \n",
    "* Aprendizaje con `MultinomialNB`\n",
    "* Mostrar algunas clasificaciones sobre sms concretos.\n",
    "* Rendimiento sobre entrenamiento y prueba.\n",
    "* Ajuste manual del parámetro de suavizado\n",
    "* Vectorización con `min_df` y `stop_words` \n",
    "\n",
    "**Nota**: este conjunto de datos no es balanceado (la mayoría son *ham*). Por tanto, usar `score` no es muy ilustrativo del rendimiento, ya que un clasificador \"tonto\" que siempre predijera *ham* tendría un rendimiento alto. Por ello, en este caso también se hace necesario usar el método `confusion_matrix` del módulo `metrics`. Se pide también explicar la salida que proporciona dicha métrica.\n",
    "\n",
    "Se pide **comentar adecuadamente cada paso realizado**, relacionándolo con lo visto en la teoría. En particular, se pide mostrar parte de los atributos `class_count_`, `class_log_prior_`, `feature_count_` y `feature_log_prob_`, explicando claramente qué son cada uno de ellos. Explicar también cómo realiza las predicciones el modelo aprendido, tal y como se ha explicado en la teoría.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación de los textos en entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos los datos del archivo de texo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataTable = pd.read_table(\"SMSSpamCollection\", names=(\"type\", \"text\"))\n",
    "dataTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear el método selectData() que divide los datos en train y test según el porcentaje seleccionado. Tendrá 3 parámetros de entrada: \n",
    "1. percentage: valor entre 0 y 1 que marca el porcentaje de los datos que serán usados para el entrenamiento. El porcentaje restante de los datos será destinado a pruebas.\n",
    "\n",
    "\n",
    "2. data: aquí se pasa el dataframe con el total de los datos que serán procesados.\n",
    "\n",
    "\n",
    "3. shuffleOrder: este valor booleano determina si el orden de los datos se baraja al iniciar el método. Si está a True los datos se desordenarán de forma que nos aseguraremos de que los datos de entrenamiento y pruebas no sean los mismos cada vez que los determinamos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def selectData(percentage, data, shuffleOrder):\n",
    "    if(shuffleOrder):\n",
    "        data = shuffle(data).reset_index(drop=True)\n",
    "    #Separamos los datos según su tipo.    \n",
    "    auxSpamData = data.loc[data['type'] == 'spam']\n",
    "    auxHamData = data.loc[data['type'] == 'ham'] \n",
    "    #Sacamos la cantidad de datos de cada tipo.\n",
    "    count_spam = auxSpamData.shape[0]\n",
    "    count_ham = auxHamData.shape[0]\n",
    "    #Y usamos esas cantidades como índices multiplicadas por el porcentaje introducido como atributo.\n",
    "    indexSpam = math.trunc(count_spam*percentage)\n",
    "    indexHam = math.trunc(count_ham*percentage)\n",
    "    trainData = [auxHamData[:indexHam], auxSpamData[:indexSpam]]\n",
    "    testData = [auxHamData[indexHam:count_ham], auxSpamData[indexSpam:count_spam]]\n",
    "    #Acabamos retornando una tupla con dos dataframes, el de los datos de entrenamiento y el de los datos de prueba.\n",
    "    allData = [trainData, testData]\n",
    "    \n",
    "    return allData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usemos el método para separar los textos de entranamiento y prueba. El 80% será de entrenamiento y el 20% de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATOS DE ENTRENAMIENTO:\n",
      "[     type                                               text\n",
      "0     ham  Go until jurong point, crazy.. Available only ...\n",
      "1     ham                      Ok lar... Joking wif u oni...\n",
      "3     ham  U dun say so early hor... U c already then say...\n",
      "4     ham  Nah I don't think he goes to usf, he lives aro...\n",
      "6     ham  Even my brother is not like to speak with me. ...\n",
      "...   ...                                                ...\n",
      "4457  ham  If you want to mapquest it or something look u...\n",
      "4458  ham  Aight should I just plan to come up later toni...\n",
      "4459  ham  Die... I accidentally deleted e msg i suppose ...\n",
      "4461  ham  This is wishing you a great day. Moji told me ...\n",
      "4462  ham  Thanks again for your reply today. When is ur ...\n",
      "\n",
      "[3860 rows x 2 columns],       type                                               text\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
      "8     spam  WINNER!! As a valued network customer you have...\n",
      "9     spam  Had your mobile 11 months or more? U R entitle...\n",
      "11    spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
      "...    ...                                                ...\n",
      "4371  spam  Do you want a new Video handset? 750 any time ...\n",
      "4373  spam  Ur balance is now £600. Next question: Complet...\n",
      "4376  spam  Ur TONEXS subscription has been renewed and yo...\n",
      "4377  spam  If you don't, your prize will go to another cu...\n",
      "4386  spam  Do you want a New Nokia 3510i Colour Phone Del...\n",
      "\n",
      "[597 rows x 2 columns]]\n",
      "DATOS DE PRUEBA:\n",
      "[     type                                               text\n",
      "4463  ham  Sorry I flaked last night, shit's seriously go...\n",
      "4464  ham  He said i look pretty wif long hair wat. But i...\n",
      "4465  ham       Ranjith cal drpd Deeraj and deepak 5min hold\n",
      "4466  ham  CHEERS FOR CALLIN BABE.SOZI CULDNT TALKBUT I W...\n",
      "4467  ham                            Hey u still at the gym?\n",
      "...   ...                                                ...\n",
      "5565  ham                                       Huh y lei...\n",
      "5568  ham               Will ü b going to esplanade fr home?\n",
      "5569  ham  Pity, * was in mood for that. So...any other s...\n",
      "5570  ham  The guy did some bitching but I acted like i'd...\n",
      "5571  ham                         Rofl. Its true to its name\n",
      "\n",
      "[965 rows x 2 columns],       type                                               text\n",
      "4394  spam  RECPT 1/3. You have ordered a Ringtone. Your o...\n",
      "4407  spam  As one of our registered subscribers u can ent...\n",
      "4410  spam  For your chance to WIN a FREE Bluetooth Headse...\n",
      "4436  spam  Don't b floppy... b snappy & happy! Only gay c...\n",
      "4450  spam  Urgent UR awarded a complimentary trip to Euro...\n",
      "...    ...                                                ...\n",
      "5537  spam  Want explicit SEX in 30 secs? Ring 02073162414...\n",
      "5540  spam  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
      "5547  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
      "5566  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
      "\n",
      "[150 rows x 2 columns]]\n"
     ]
    }
   ],
   "source": [
    "completeData = selectData(0.8, dataTable, False)\n",
    "\n",
    "print('DATOS DE ENTRENAMIENTO:')\n",
    "print(completeData[0])\n",
    "\n",
    "print('DATOS DE PRUEBA:')\n",
    "print(completeData[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización de los textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a vectorizar los datos. El primer paso será sacar los datos de nuestro dataFrame y crear varias listas en las que separaremos por un lado los textos de los mensajes y por otro sus respectivos tipos de forma ordenada para que la relación entre listas sea directa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  4457 4457 test size:  1115 1115\n"
     ]
    }
   ],
   "source": [
    "#text_train contendrá los textos de los datos de entrenamiento, incluyendo indiferentemente tipos ham y spam.\n",
    "text_train = completeData[0][0]['text'].tolist()\n",
    "text_train.extend(completeData[0][1]['text'].tolist())\n",
    "\n",
    "#y_train será la lista ordenada de tipos de los mensajes, en nuestro caso aparecerán primero todas las instancias ham y \n",
    "# seguidamente todas las instacias spam.\n",
    "y_train = completeData[0][0]['type'].tolist()\n",
    "y_train.extend(completeData[0][1]['type'].tolist())\n",
    "\n",
    "#text_test equivalente a text_train para datos de pruebas.\n",
    "text_test = completeData[1][0]['text'].tolist()\n",
    "text_test.extend(completeData[1][1]['text'].tolist())\n",
    "\n",
    "#y_test equivalente a y_train para datos de pruebas\n",
    "y_test = completeData[1][0]['type'].tolist()\n",
    "y_test.extend(completeData[1][1]['type'].tolist())\n",
    "\n",
    "#Como podemos comprobar tenemos dos listas de entrenamiento de tamaño 4457 (80% de los datos) y dos listas contenedoras\n",
    "# de la información de prueba con tamaño 1115 (20% de los datos).\n",
    "print(\n",
    "\"train size: \", len(text_train),\n",
    "len(y_train),\n",
    "\"test size: \", len(text_test),\n",
    "len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos obtenido y ordenado los datos en un formato adecuado, a continuación los procesaremos vectorialmente y haremos una serie de comprobaciones. Para ello haremos uso del transformador CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<4457x7776 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 59601 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Le pasamos los textos de entrenamiento con el método fit(), de esta manera se recopila el vocabulario.\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "\n",
    "#Con el método transform() conseguiremos el formato de vector en base a los textos.\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y a continuación podemos ver la representación de esta matriz de vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representación no dispersa de los documentos del ejemplo:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Representación no dispersa de los documentos del ejemplo:\\n{}\".format(X_train.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analicemos en una primera instancia el vocabulario generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de términos en el vocabulario: 7776\n",
      "Primeras 20 características (términos):\n",
      "['00', '000', '000pes', '008704050406', '0089', '01223585236', '01223585334', '0125698789', '02', '0207', '02072069400', '02085076972', '021', '03', '04', '0430', '05', '050703', '0578', '06']\n",
      "Términos del 6990 al 7000:\n",
      "['tonght', 'tongued', 'tonight', 'tonights', 'tonite', 'tons', 'too', 'took', 'tookplace', 'tool']\n",
      "Términos cada 500 posiciones:\n",
      "['00', '4742', 'apology', 'bribe', 'copied', 'earlier', 'french', 'horo', 'kvb', 'milk', 'oranges', 'proverb', 'secretly', 'sterling', 'tooo', 'what']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Número de términos en el vocabulario: {}\".format(len(feature_names)))\n",
    "print(\"Primeras 20 características (términos):\\n{}\".format(feature_names[:20]))\n",
    "print(\"Términos del 6990 al 7000:\\n{}\".format(feature_names[6990:7000]))\n",
    "print(\"Términos cada 500 posiciones:\\n{}\".format(feature_names[::500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar el vocabulario contiene 7776 palabras, a primera vista podemos observar que muchas de ellas se refieren al mismo término pero conteniendo algún tipo de falta de ortografía. Esto era de esperar dado el origen de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje con MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comenzar el entrenamiento mediante el Naive Bayes Multinomial. Como sabemos, el modelo multinomial tiene en cuenta el número de ocurrencias de los términos. Además la ausencia de un término no influye en la clasificación por lo que será un método muy adecuado para el tipo de uso que queremos darle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Regresión logística con el parámetro por defecto\n",
    "multinb=MultinomialNB().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esa llamada al método fit() y pasando nuestros datos vectorizados de entrenamiento, el modelo NB cuenta el número de instancias de los términos encontrados en cada clase (ham, spam) y guarda esta información en forma de logaritmo de las proporciones en una serie de atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación pasemos a detallar parte de la información sobre algunos de esos atributos en el método:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con **class_count_** podemos comprobar el número de cada clase. Por supuesto sigue siendo el mismo que marcamos al principio de este documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3860.  597.]\n"
     ]
    }
   ],
   "source": [
    "print(multinb.class_count_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con **class_log_prior_** podemos saber las probabilidades logarítmicas a priori de cada clase del modelo.  \n",
    "\n",
    "Esto se calcula de la siguiente manera:\n",
    "\n",
    "&emsp; $$P(Causa|Efecto) = \\displaystyle \\Bigg[\\frac{P(Efecto|Causa)P(Efecto)}{P(Causa))}\\Bigg]$$\n",
    "\n",
    "Siendo:\n",
    "\n",
    "        1. P(Efecto): La probabilidad que tiene el Efecto de ocurrir.\n",
    "        2. P(Efecto|Causa): La probabilidad de que el Efecto ocurra dada la Causa como cierta.\n",
    "        3. P(Causa|Efecto): La probabilidad de que la Causa ocurra dado el Efecto como cierto.\n",
    "        4. P(Causa): La probabilidad que tiene la Causa de ocurrir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14380871 -2.01031406]\n"
     ]
    }
   ],
   "source": [
    "print(multinb.class_log_prior_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con **feature_count_** se muestra el número de repeticiones de instancias de cada término. Se muestran dos listas, una por cada tipo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1. ...  1.  1.  0.]\n",
      " [10. 25.  0. ...  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(multinb.feature_count_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con **feature_log_prob_** tenemos la lista de las probabilidades logarítmicas de las ocurrencias de cada término respecto a todos los demás. Con el contenido de estas listas (una por tipo) se realizarán las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.97594408 -10.97594408 -10.28279689 ... -10.28279689 -10.28279689\n",
      "  -10.97594408]\n",
      " [ -7.59135705  -6.73115578  -9.98925232 ...  -9.98925232  -9.98925232\n",
      "   -9.29610514]]\n",
      "Uno por cada palabra del vocabulario y lista:  7776.0\n"
     ]
    }
   ],
   "source": [
    "print(multinb.feature_log_prob_)\n",
    "\n",
    "print(\"Uno por cada palabra del vocabulario y lista: \",multinb.feature_log_prob_.size/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para explicar de forma teórica cómo realiza el modelo las predicciones debemos tener en cuenta las listas vistas recientemente como una serie de atributos de nuestras instancias que acabarán determinando el valor de clasificación con mayor probabilidad de acierto. \n",
    "\n",
    "Tenemos en primer lugar la probabilidad incondicional de pertenecer a una clase, es decir, todos nuestro sms serán necesariamente \"ham\" o \"spam\". Esta probabilidad irá ligada a la cantidad de ejemplos de cada tipo en los datos de entrenamiento como observábamos anteriormente.\n",
    "\n",
    "Por otra parte las probabilidades de aparición de una palabra del vocabulario dado por cierto que se es de una determinada clase serán condicionalmente independientes. Esto lo damos por hecho para contar con que la clase contiene toda la información necesaria acerca de cada atributo y poder simplificar así el problema. De otra manera necesitaríamos una cantidad ingente de datos (instancias de todas las combinaciones). \n",
    "\n",
    "Teniendo en cuenta lo anterior podemos saber que las probabilidades del conjunto de atributos dada la clase se pueden factorizar, es decir, tendremos un conjunto de factores (uno por palabra) cuya maximización de producto determinará a qué clase o tipo se predice que pertenece la instancia.\n",
    "\n",
    "Como conclusión ahora sabemos que, teniendo las listas **feature_log_prob_** (factores de los que hablamos), tendremos un factor posible que añadir al producto según si la palabra aparece o no en la instancia. Si este producto es mayor para el tipo \"ham\" el mensaje se predice como \"ham\". De ser mayor el producto \"spam\" se marcará como \"spam\". Por supuesto, aunque necesitemos marcar un único tipo categórico siempre podremos analizar la fiabilidad de la predicción hecha según las dos probabilidades calculadas y su aproximación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrar algunas clasificaciones sobre sms concretos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos el modelo en funcionamiento, hagamos alguna comprobaciones para terminar de entenderlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar saquemos la lista de datos de prueba un par de ejemplo arbitraios con sus respectivos tipos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segundo SMS del conjunto de test: \n",
      "\n",
      "He said i look pretty wif long hair wat. But i thk he's cutting quite short 4 me leh.\n",
      "\n",
      "Clasificación verdadera: ham.\n",
      "\n",
      "\n",
      "Milunésimo SMS del conjunto de test: \n",
      "\n",
      "URGENT! Your Mobile No 07808726822 was awarded a £2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\n",
      "\n",
      "Clasificación verdadera: spam\n"
     ]
    }
   ],
   "source": [
    "print(\"Segundo SMS del conjunto de test: \\n\\n{}\\n\".format(text_test[1]))\n",
    "print(\"Clasificación verdadera: {}.\\n\\n\".format(y_test[1]))\n",
    "\n",
    "print(\"Milunésimo SMS del conjunto de test: \\n\\n{}\\n\".format(text_test[1000]))\n",
    "print(\"Clasificación verdadera: {}\".format(y_test[1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple vista se puede observar la amplia diferencia entre un mensaje tipo \"ham\" y uno \"spam\". Veamos qué dice nuestro modelo al preguntar por una predicción para estos mismos mensajes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del clasificador para el segundo SMS: ham\n",
      "\n",
      "Predicción del clasificador para el milunésimo SMS: spam\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicción del clasificador para el segundo SMS: {}\\n\".format(multinb.predict(vect.transform([text_test[1]]))[0]))\n",
    "\n",
    "print(\"Predicción del clasificador para el milunésimo SMS: {}\".format(multinb.predict(vect.transform([text_test[1000]]))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente, nuestro modelo ha sido capaz de predecir satisfactoriamente los tipos de estos mensajes. Analicemos con cuánta seguridad se han dado estas predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción de probabilidad para el segundo SMS: [1.00000000e+00 5.78325665e-18]\n",
      "\n",
      "Predicción de probabilidad para el milunésimo SMS: [4.14715943e-20 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicción de probabilidad para el segundo SMS: {}\\n\".format(multinb.predict_proba(vect.transform([text_test[1]]))[0]))\n",
    "\n",
    "print(\"Predicción de probabilidad para el milunésimo SMS: {}\".format(multinb.predict_proba(vect.transform([text_test[1000]]))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podría decirse que la seguridad es prácticamente completa en los dos casos, tipo \"ham\" el primero y \"spam\" el segundo. Si volvemos a los mensajes propiamente dichos puede corroborarse que contienen palabras que alguien esperaría en mensajes de cada tipo, siendo llamativa la cantidad de veces que en los mensajes de \"spam\" se usan palabras relacionadas a \"ganar premios\", \"llamar de forma urgente\" o \"enviar algo a una dirección\". Vamos a experimentar un poco con este concepto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué ocurre si inventamos un nuevo mensaje y le pedimos al modelo que dé una predicción. Vamos a usar dos palabras que con total seguridad aparecen normalmente en mensajes de tipo spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veces que 'win' aparece en los datos de entrenamiento de tipo 'ham': 9.0\n",
      "Veces que 'win' aparece en los datos de entrenamiento de tipo 'spam': 53.0\n",
      "Veces que 'now' aparece en los datos de entrenamiento de tipo 'ham': 0.0\n",
      "Veces que 'now' aparece en los datos de entrenamiento de tipo 'spam': 76.0\n"
     ]
    }
   ],
   "source": [
    "mensaje1 = \"win prize\"\n",
    "\n",
    "print(\"Veces que 'win' aparece en los datos de entrenamiento de tipo 'ham':\", multinb.feature_count_[0][feature_names.index(\"win\")])\n",
    "print(\"Veces que 'win' aparece en los datos de entrenamiento de tipo 'spam':\", multinb.feature_count_[1][feature_names.index(\"win\")])\n",
    "\n",
    "print(\"Veces que 'now' aparece en los datos de entrenamiento de tipo 'ham':\", multinb.feature_count_[0][feature_names.index(\"prize\")])\n",
    "print(\"Veces que 'now' aparece en los datos de entrenamiento de tipo 'spam':\", multinb.feature_count_[1][feature_names.index(\"prize\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra suposición era cierta, las palabras \"win\" y \"prize\" apenas tienen instancias entre los mensajes de tipo 'ham' usados en el entrenamiento. Y ninguna instancia, de hecho, para la palabra \"prize\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué predice el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción de mensaje inventado: spam\n",
      "Predicción de probabilidad del mensaje inventado: [0.00215656 0.99784344]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicción de mensaje inventado: {}\".format(multinb.predict(vect.transform([mensaje1]))[0]))\n",
    "print(\"Predicción de probabilidad del mensaje inventado: {}\\n\".format(multinb.predict_proba(vect.transform([mensaje1]))[0]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La probabilidad de que el mensaje sea 'spam' según el modelo es abrumadora. Esto concuerda perfectamente con la lógica entendida hasta el momento sobre el funcionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué ocurre en un caso más ambiguo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veces que 'hello' aparece en los datos de entrenamiento de tipo 'ham': 40.0\n",
      "Veces que 'hello' aparece en los datos de entrenamiento de tipo 'spam': 4.0\n",
      "Veces que 'urgent' aparece en los datos de entrenamiento de tipo 'ham': 6.0\n",
      "Veces que 'urgent' aparece en los datos de entrenamiento de tipo 'spam': 49.0\n"
     ]
    }
   ],
   "source": [
    "mensaje2 = \"hello urgent\"\n",
    "\n",
    "print(\"Veces que 'hello' aparece en los datos de entrenamiento de tipo 'ham':\", multinb.feature_count_[0][feature_names.index(\"hello\")])\n",
    "print(\"Veces que 'hello' aparece en los datos de entrenamiento de tipo 'spam':\", multinb.feature_count_[1][feature_names.index(\"hello\")])\n",
    "\n",
    "print(\"Veces que 'urgent' aparece en los datos de entrenamiento de tipo 'ham':\", multinb.feature_count_[0][feature_names.index(\"urgent\")])\n",
    "print(\"Veces que 'urgent' aparece en los datos de entrenamiento de tipo 'spam':\", multinb.feature_count_[1][feature_names.index(\"urgent\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta ocasión la palabra \"hello\" es claramente predominante en los mensajes 'ham' y la palabra \"urgent\" lo es en los mensajes 'spam'. Veamos cómo reacciona nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción de mensaje inventado: ham\n",
      "Predicción de probabilidad del mensaje inventado: [0.50778515 0.49221485]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicción de mensaje inventado: {}\".format(multinb.predict(vect.transform([\"hello urgent\"]))[0]))\n",
    "print(\"Predicción de probabilidad del mensaje inventado: {}\".format(multinb.predict_proba(vect.transform([\"hello urgent\"]))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como era de esperar, la predicción ha sido completamente igualada ganando por algunos puntos la predicción 'ham' y de ahí el tipo predicho. Esto es completamente lógico puesto que a efectos prácticos en nuestro vector existen dos factores contrarios cuyo peso es aproximadamente cercarno para nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto con estos últimos ejemplos no se pretende mostrar un uso correcto o convencional del modelo probabilístico sino más bien comprenderlo más a fondo en base a reproducir instancias extremas poco realistas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendimiento sobre entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar la función score() para ver el porcentaje de acierto en las predicciones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos la medición tanto con los datos de entrenamiento como con los de prueba aunque estos últimos serán los más significativos para medir el éxito de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento de multinb sobre el conjunto de entrenamiento: 0.99\n",
      "Rendimiento de multinb sobre el conjunto de test: 0.98\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "\n",
    "print(\"Rendimiento de multinb sobre el conjunto de entrenamiento: {:.2f}\".format(multinb.score(X_train,y_train)))\n",
    "print(\"Rendimiento de multinb sobre el conjunto de test: {:.2f}\".format(multinb.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que el modelo consigue un porcentaje muy alto de acierto, lo cual es bueno pero debemos tener en cuenta una consideración extra. Recordemos que las clases de nuestros datos no están repartidas de manera uniforme, existen muchos más ejemplos de mensaje clasificados como 'ham' que mensajes clasificados como 'spam'.\n",
    "\n",
    "Es por esto que no debemos preocuparnos únicamente por el porcentaje absoluto de acierto sino que además tendremos que investigar de dónde salen exáctamente los errores que comete el modelo.\n",
    "\n",
    "Para ello haremos uso de una matrix de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x20c87ada400>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "# Montamos una lista de las clases ordenadas de las predicciones hechas por el modelo en base a los datos de prueba.\n",
    "for i in range(len(y_test)):\n",
    "    aux = multinb.predict(vect.transform([text_test[i]]))\n",
    "    y_pred.append(aux) \n",
    "\n",
    "# El método recibirá dos listas. La lista de valores correcto (y_test) y la lista de predicciones \n",
    "# del modelo respecto a esas instancias (y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Por último dibujamos la matriz añadiendo las correspondientes etiquetas.\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"ham\",\"spam\"])\n",
    "\n",
    "disp.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según el número de clases de los datos que tratamos de analizar, la matriz de confunsión tendrá una dimensión determinada. En este caso, con dos clases se tienen cuatro posiciones a las que llamaremos [(0,0), (0,1), (1,0), (1,1)].\n",
    "   1. Posición (0,0): con valor 956, número de ocasiones en que se ha predicho 'ham' y se ha acertado.\n",
    "   2. Posición (0,1): con valor 9, número de ocasiones en que se ha predicho 'spam' y se ha fallado.\n",
    "   3. Posición (1,0): con valor 8, número de ocasiones en que se ha predicho 'ham' y se ha fallado.\n",
    "   4. Posición (1,1): con valor 142, número de ocasiones en que se ha predicho 'spam' y se ha acertado.\n",
    "   \n",
    "Ahora poseemos más información para analizar el comportamiento del modelo. Podemos observar que los errores (posiciones (0,1) y (1,0)) no se concentran especialmente en una de las posiciones. Si por ejemplo todos los errores se concentrasen en (0,1) y en gran cantidad, sería evidente que el modelo acierta principalmente por predecir casi siempre 'ham' y la cantidad mayoritaria de aciertos llegaría por haber un mayor número de casos 'ham' entre el groso de datos y no por tener un modelo eficaz.  \n",
    "\n",
    "También puede destacarse que aunque haya una cantidad parecida de errores de predicción de cada tipo, no deben tenerse en consideración de la misma manera, es decir, no tendrá la misma importancia un error por predicción de 'ham' para el que poseemos muchas más instancias concretas que un error por predicción de 'spam', del que disponíamos en menor cantidad durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste manual del parámetro de suavizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El suavizado de las estimaciones es un tratamiento que se realiza a los datos con objetivo de solventar el problema que surje cuando no tenemos instancias de algún tipo y la predicción hacia un lado alcanza valores de 0 o extremádamente bajos. El método MultinomialNB() nos permite hacer uso de un parámetro 'alpha', el parámetro de suavizado. Este parámetro es 1.0 por defecto pero al modificarlo podremos observar cambios en el rendimiento del modelo y nos posibilitará evitar el sobreajuste en las predicciones.\n",
    "\n",
    "Como aprendimos en teoría, el parámetro de suavizado es fundamentalmente una constante que se suma en los diversos cálculos de probabilidades del modelo. Será necesario calibrar a mano el valor más adecuado para este parámetro.\n",
    "\n",
    "Empecemos dándole un valor alpha=10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento de multinb sobre el conjunto de entrenamiento 0.96\n",
      "Rendimiento de multinb sobre el conjunto de test: 0.96\n"
     ]
    }
   ],
   "source": [
    "multinb_alpha=MultinomialNB(alpha=10).fit(X_train,y_train)\n",
    "print(\"Rendimiento de multinb sobre el conjunto de entrenamiento {:.2f}\".format(multinb_alpha.score(X_train,y_train)))\n",
    "print(\"Rendimiento de multinb sobre el conjunto de test: {:.2f}\".format(multinb_alpha.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el rendimiento es menor. Para seleccionar el mejor valor de suavizado podemos hacer uso del mnétodo GridSearchCV(). Se trata de un modelo predictivo en sí que nos devolverá el mejor entre varios valores posibles de alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor parámetro:  {'alpha': 0.01}\n",
      "Rendimiento de MultonomialNB en validación cruzada, con el mejor parámetro: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid_nb = {'alpha': [0.0001,0.001, 0.01,0.1, 1, 10,100,200]} # Posibles valores\n",
    "grid_nb = GridSearchCV(MultinomialNB(), param_grid_nb, cv=5)\n",
    "grid_nb.fit(X_train, y_train) # Entrenamiento\n",
    "print(\"Mejor parámetro: \", grid_nb.best_params_)\n",
    "print(\"Rendimiento de MultonomialNB en validación cruzada, con el mejor parámetro: {:.2f}\".format(grid_nb.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que el mejor valor encontrado para alpha es 0.01 y como observamos a continuación, tener un parámetro de suavizado bajo no va en detrimento de la efectividad del modelo. Necesita poco suavizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento sobre prueba (del mejor parámetro en validación cruzada): 0.98\n"
     ]
    }
   ],
   "source": [
    "print(\"Rendimiento sobre prueba (del mejor parámetro en validación cruzada): {:.2f}\".format(grid_nb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización con min_df y stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una idea impoortante a tener en cuenta en este tipo de modelos es que no necesariamente todas las palabras de nuestro vocabulario son relevantes para las predicciones. Esto es debido a que existen palabras de uso tan común y además con un significado tan neutro que no nos ayudarán a determinar el tipo de un mensaje. Algunos ejemplos de este tipo de palabras serían 'que', 'de', 'of' o 'the'. Por supuesto según el idioma tendremos palabras distintas, denominadas \"stop words\".\n",
    "\n",
    "Por otra parte también será interesante descartar palabras cuya cantidad de apariciones en los datos sea relativamente baja. Para esto usaremos \"min_df\" que marca el rango de cantidad de instancias a partir del cual se seguirá teniendo en cuenta el elemento. Se debe escoger un \"min_df\" adecuado en función a la cantidad de instancias totales de nuestros datos.\n",
    "\n",
    "Algo importante a este respecto es que el objetivo de volver a vectorizar nuestros datos será una cuestión de eficiencia puesto que estaremos eliminando una gran cantidad de información redundante (mayor eficiencia) pero no estamos añadiendo ninguna nueva (igual desempeño).\n",
    "\n",
    "Vamos a usar el método CountVectorizer(), al que indicaremos un valor min_df=10 y stop_words=\"english\" para que tenga en cuenta las palabras más cotidianas del habla inglesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect2 = CountVectorizer(min_df=10, stop_words=\"english\").fit(text_train)\n",
    "X2_train = vect2.transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de términos en el vocabulario original: 7776\n",
      "Número de términos en el vocabulario con stop words y min_df: 693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '03',\n",
       " '0800',\n",
       " '08000839402',\n",
       " '08000930705',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10p',\n",
       " '11',\n",
       " '12hrs',\n",
       " '150',\n",
       " '150p',\n",
       " '150ppm',\n",
       " '16',\n",
       " '18',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '250',\n",
       " '2lands',\n",
       " '2nd',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '750',\n",
       " '800',\n",
       " '8007',\n",
       " '86688',\n",
       " '87066',\n",
       " 'abiola',\n",
       " 'able',\n",
       " 'abt',\n",
       " 'account',\n",
       " 'actually',\n",
       " 'address',\n",
       " 'aft',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'ah',\n",
       " 'aight',\n",
       " 'alright',\n",
       " 'amp',\n",
       " 'angry',\n",
       " 'ans',\n",
       " 'answer',\n",
       " 'anytime',\n",
       " 'apply',\n",
       " 'ard',\n",
       " 'area',\n",
       " 'ask',\n",
       " 'askd',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'ass',\n",
       " 'attempt',\n",
       " 'available',\n",
       " 'await',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'b4',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'bad',\n",
       " 'balance',\n",
       " 'bank',\n",
       " 'bcoz',\n",
       " 'beautiful',\n",
       " 'bed',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'blue',\n",
       " 'bonus',\n",
       " 'book',\n",
       " 'bored',\n",
       " 'bout',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boytoy',\n",
       " 'break',\n",
       " 'bring',\n",
       " 'brother',\n",
       " 'bslvyl',\n",
       " 'bt',\n",
       " 'bus',\n",
       " 'busy',\n",
       " 'buy',\n",
       " 'call2optout',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'camcorder',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'carlos',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cause',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'chat',\n",
       " 'check',\n",
       " 'chennai',\n",
       " 'chikku',\n",
       " 'choose',\n",
       " 'christmas',\n",
       " 'claim',\n",
       " 'class',\n",
       " 'close',\n",
       " 'club',\n",
       " 'code',\n",
       " 'collect',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'colour',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comin',\n",
       " 'coming',\n",
       " 'company',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'cool',\n",
       " 'correct',\n",
       " 'cos',\n",
       " 'cost',\n",
       " 'couple',\n",
       " 'coz',\n",
       " 'crazy',\n",
       " 'credit',\n",
       " 'cs',\n",
       " 'cum',\n",
       " 'currently',\n",
       " 'customer',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'darlin',\n",
       " 'dat',\n",
       " 'date',\n",
       " 'dating',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dear',\n",
       " 'decided',\n",
       " 'decimal',\n",
       " 'delivery',\n",
       " 'den',\n",
       " 'details',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'difficult',\n",
       " 'dinner',\n",
       " 'direct',\n",
       " 'dis',\n",
       " 'dnt',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doin',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'dont',\n",
       " 'double',\n",
       " 'download',\n",
       " 'draw',\n",
       " 'dreams',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'driving',\n",
       " 'drop',\n",
       " 'dude',\n",
       " 'dun',\n",
       " 'dunno',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eh',\n",
       " 'email',\n",
       " 'end',\n",
       " 'ends',\n",
       " 'enjoy',\n",
       " 'enter',\n",
       " 'entered',\n",
       " 'entry',\n",
       " 'eve',\n",
       " 'evening',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'family',\n",
       " 'fancy',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'felt',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'food',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'fr',\n",
       " 'free',\n",
       " 'freemsg',\n",
       " 'fri',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'friendship',\n",
       " 'frnds',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'fun',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gd',\n",
       " 'getting',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'goodmorning',\n",
       " 'got',\n",
       " 'gotta',\n",
       " 'gr8',\n",
       " 'great',\n",
       " 'gt',\n",
       " 'guaranteed',\n",
       " 'gud',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'ha',\n",
       " 'haf',\n",
       " 'haha',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hav',\n",
       " 'haven',\n",
       " 'havent',\n",
       " 'having',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'hee',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hey',\n",
       " 'hg',\n",
       " 'hi',\n",
       " 'hit',\n",
       " 'hmm',\n",
       " 'hmmm',\n",
       " 'hold',\n",
       " 'holiday',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'hows',\n",
       " 'http',\n",
       " 'huh',\n",
       " 'hungry',\n",
       " 'hurt',\n",
       " 'id',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'immediately',\n",
       " 'important',\n",
       " 'info',\n",
       " 'information',\n",
       " 'invited',\n",
       " 'isn',\n",
       " 'jay',\n",
       " 'job',\n",
       " 'join',\n",
       " 'jus',\n",
       " 'just',\n",
       " 'juz',\n",
       " 'kind',\n",
       " 'kiss',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'land',\n",
       " 'landline',\n",
       " 'laptop',\n",
       " 'lar',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'leave',\n",
       " 'leaves',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'leh',\n",
       " 'lei',\n",
       " 'lesson',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'liao',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'line',\n",
       " 'listen',\n",
       " 'little',\n",
       " 'live',\n",
       " 'll',\n",
       " 'loads',\n",
       " 'log',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'lor',\n",
       " 'lose',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'lovely',\n",
       " 'loving',\n",
       " 'lt',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'luv',\n",
       " 'mah',\n",
       " 'mail',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'march',\n",
       " 'mate',\n",
       " 'mates',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meant',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'merry',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'met',\n",
       " 'min',\n",
       " 'mind',\n",
       " 'mins',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'missed',\n",
       " 'missing',\n",
       " 'mm',\n",
       " 'mob',\n",
       " 'mobile',\n",
       " 'mobileupd8',\n",
       " 'mom',\n",
       " 'money',\n",
       " 'month',\n",
       " 'morning',\n",
       " 'motorola',\n",
       " 'movie',\n",
       " 'mr',\n",
       " 'mrng',\n",
       " 'msg',\n",
       " 'msgs',\n",
       " 'mu',\n",
       " 'mum',\n",
       " 'music',\n",
       " 'na',\n",
       " 'nah',\n",
       " 'national',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'net',\n",
       " 'network',\n",
       " 'neva',\n",
       " 'new',\n",
       " 'news',\n",
       " 'ni8',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'nite',\n",
       " 'noe',\n",
       " 'nokia',\n",
       " 'noon',\n",
       " 'nope',\n",
       " 'nt',\n",
       " 'ntt',\n",
       " 'number',\n",
       " 'nyt',\n",
       " 'offer',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'okie',\n",
       " 'old',\n",
       " 'online',\n",
       " 'open',\n",
       " 'operator',\n",
       " 'opt',\n",
       " 'orange',\n",
       " 'order',\n",
       " 'oredi',\n",
       " 'oso',\n",
       " 'outside',\n",
       " 'pa',\n",
       " 'pain',\n",
       " 'parents',\n",
       " 'park',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'pay',\n",
       " 'people',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'phones',\n",
       " 'pic',\n",
       " 'pick',\n",
       " 'picking',\n",
       " 'pics',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'plans',\n",
       " 'play',\n",
       " 'player',\n",
       " 'pls',\n",
       " 'plus',\n",
       " 'plz',\n",
       " 'pm',\n",
       " 'po',\n",
       " 'pobox',\n",
       " 'point',\n",
       " 'points',\n",
       " 'poly',\n",
       " 'post',\n",
       " 'pounds',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'price',\n",
       " 'princess',\n",
       " 'private',\n",
       " 'prize',\n",
       " 'prob',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'project',\n",
       " 'pub',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quite',\n",
       " 'quiz',\n",
       " 'rate',\n",
       " 'reach',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'receive',\n",
       " 'remember',\n",
       " 'reply',\n",
       " 'right',\n",
       " 'ring',\n",
       " 'ringtone',\n",
       " 'rite',\n",
       " 'room',\n",
       " 'row',\n",
       " 'rply',\n",
       " 'run',\n",
       " 'sad',\n",
       " 'sae',\n",
       " 'said',\n",
       " 'sat',\n",
       " 'saturday',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'sch',\n",
       " 'school',\n",
       " 'sea',\n",
       " 'search',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'seeing',\n",
       " 'selected',\n",
       " 'send',\n",
       " 'sending',\n",
       " 'sent',\n",
       " 'service',\n",
       " 'services',\n",
       " 'set',\n",
       " 'sexy',\n",
       " 'shall',\n",
       " 'shit',\n",
       " 'shop',\n",
       " 'shopping',\n",
       " 'shows',\n",
       " 'sir',\n",
       " 'sis',\n",
       " 'sister',\n",
       " 'sleep',\n",
       " 'sleeping',\n",
       " 'smile',\n",
       " 'smiling',\n",
       " 'smoke',\n",
       " 'sms',\n",
       " 'snow',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'speak',\n",
       " 'special',\n",
       " 'start',\n",
       " 'started',\n",
       " 'stay',\n",
       " 'stop',\n",
       " 'story',\n",
       " 'stuff',\n",
       " 'suite342',\n",
       " 'sun',\n",
       " 'sure',\n",
       " 'surprise',\n",
       " 'sweet',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'tc',\n",
       " 'tel',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'test',\n",
       " 'text',\n",
       " 'texts',\n",
       " 'th',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'thats',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'thinks',\n",
       " 'thk',\n",
       " 'tho',\n",
       " 'thought',\n",
       " 'til',\n",
       " 'till',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tired',\n",
       " 'tmr',\n",
       " 'today',\n",
       " 'todays',\n",
       " 'told',\n",
       " 'tomo',\n",
       " 'tomorrow',\n",
       " 'tone',\n",
       " 'tones',\n",
       " 'tonight',\n",
       " 'took',\n",
       " 'tot',\n",
       " 'touch',\n",
       " 'town',\n",
       " 'treat',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'ts',\n",
       " 'tv',\n",
       " 'txt',\n",
       " 'txts',\n",
       " 'type',\n",
       " 'ugh',\n",
       " 'uk',\n",
       " 'understand',\n",
       " 'unlimited',\n",
       " 'unsubscribe',\n",
       " 'update',\n",
       " 'ur',\n",
       " 'urgent',\n",
       " 'use',\n",
       " 'used',\n",
       " 'valid',\n",
       " 'valued',\n",
       " 've',\n",
       " 'video',\n",
       " 'visit',\n",
       " 'voucher',\n",
       " 'vouchers',\n",
       " 'wait',\n",
       " 'waiting',\n",
       " 'wake',\n",
       " 'walk',\n",
       " 'wan',\n",
       " 'wana',\n",
       " 'wanna',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'wat',\n",
       " 'watch',\n",
       " 'watching',\n",
       " 'way',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weekly',\n",
       " 'weeks',\n",
       " 'welcome',\n",
       " 'wen',\n",
       " 'went',\n",
       " 'whats',\n",
       " 'wid',\n",
       " 'wif',\n",
       " 'wife',\n",
       " 'wil',\n",
       " 'win',\n",
       " 'winner',\n",
       " 'wish',\n",
       " 'wit',\n",
       " 'wk',\n",
       " 'won',\n",
       " 'wonder',\n",
       " 'wonderful',\n",
       " 'wont',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'working',\n",
       " 'world',\n",
       " 'worry',\n",
       " 'worth',\n",
       " 'wot',\n",
       " 'wrong',\n",
       " 'www',\n",
       " 'xmas',\n",
       " 'xx',\n",
       " 'xxx',\n",
       " 'ya',\n",
       " 'yar',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'yo',\n",
       " 'yr',\n",
       " 'yup']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Número de términos en el vocabulario original: {}\".format(len(feature_names)))\n",
    "feature_names2 = vect2.get_feature_names()\n",
    "print(\"Número de términos en el vocabulario con stop words y min_df: {}\".format(len(feature_names2)))\n",
    "feature_names2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro vocabulario se ha reducido de 7776 palabras a 693 siendo ahora más de diez veces menor. Esto puede marcar una gran diferencia en eficiencia para modelos que manejan cantidades masivas de datos.\n",
    "\n",
    "Analicemos con este nuevo vocabulario si cambian en algo las características del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinb2=MultinomialNB(alpha=0.1).fit(X2_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento de multinb2 sobre el conjunto de entrenamiento 0.98\n",
      "Rendimiento de multinb2 sobre el conjunto de test: 0.98\n"
     ]
    }
   ],
   "source": [
    "X2_test = vect2.transform(text_test)\n",
    "\n",
    "print(\"Rendimiento de multinb2 sobre el conjunto de entrenamiento {:.2f}\".format(multinb2.score(X2_train,y_train)))\n",
    "print(\"Rendimiento de multinb2 sobre el conjunto de test: {:.2f}\".format(multinb2.score(X2_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemnos observar, la capacidad de predicción del modelo sigue intacta, ahora de forma mucho más optimizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ahora, si lo deseamos, volver a calcular el parámetro de suavizado más optimo para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor parámetro:  {'alpha': 10}\n",
      "Rendimiento de MultonomialNB (con min_df y stop words) en validación cruzada, con el mejor parámetro: 0.98\n",
      "Rendimiento sobre prueba (del mejor parámetro en validación cruzada): 0.98\n"
     ]
    }
   ],
   "source": [
    "grid2_nb = GridSearchCV(MultinomialNB(), param_grid_nb, cv=5)\n",
    "grid2_nb.fit(X2_train, y_train)\n",
    "print(\"Mejor parámetro: \", grid2_nb.best_params_)\n",
    "print(\"Rendimiento de MultonomialNB (con min_df y stop words) en validación cruzada, con el mejor parámetro: {:.2f}\".format(grid2_nb.best_score_))\n",
    "print(\"Rendimiento sobre prueba (del mejor parámetro en validación cruzada): {:.2f}\".format(grid2_nb.score(X2_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro de suavizado más adecuado en esta ocasión es alpha=10. Para que ahora nuestro modelo es algo más sensible al sobreajuste y de ahí que el valor del parámetro haya aumentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
